import sys # <-- æ–°å¢žçš„å¯¼å…¥
import streamlit as st
import streamlit.web.cli as stcli
import joblib
import pandas as pd
import os
import numpy as np
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import MinMaxScaler 
import matplotlib.pyplot as plt
import seaborn as sns
from utils import load_data, get_input_variables, get_output_variables, get_categorical_mappings, get_smiles_mappings
from rdkit import Chem
from rdkit.Chem.Draw import rdMolDraw2D
import base64
from io import BytesIO
import itertools

# --- ðŸŽ¯ è·¯å¾„ä¿®æ­£ï¼šå°†å½“å‰å·¥ä½œç›®å½•åˆ‡æ¢åˆ° app.py æ‰€åœ¨çš„æ–‡ä»¶å¤¹ ---
# è¿™ç¡®ä¿äº† 'Models/Trained' è¿™æ ·çš„ç›¸å¯¹è·¯å¾„æ˜¯æ­£ç¡®çš„
# --- ðŸŽ¯ è·¯å¾„ä¿®æ­£ï¼šå°†å½“å‰å·¥ä½œç›®å½•åˆ‡æ¢åˆ° app.py æ‰€åœ¨çš„æ–‡ä»¶å¤¹ ---
try:
    current_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(current_dir)
    import logging
    logging.warning(f"å½“å‰å·¥ä½œç›®å½•å·²åˆ‡æ¢è‡³: {os.getcwd()}")
    st.write(f"å½“å‰å·¥ä½œç›®å½•å·²åˆ‡æ¢è‡³: {os.getcwd()}") 
except Exception as e:
    print(f"æ— æ³•åˆ‡æ¢å·¥ä½œç›®å½•{e}")Â 

# åŠ è½½æ¨¡åž‹ã€æ ‡å‡†åŒ–å™¨å’Œæ•°æ®
@st.cache_resource
def load_models_and_data():
    """
    åŠ è½½æ‰€æœ‰å·²è®­ç»ƒçš„æ¨¡åž‹ã€æ ‡å‡†åŒ–å™¨å’ŒåŽŸå§‹æ•°æ®ã€‚
    åŒæ—¶æž„å»ºç”¨äºŽæ•°æ®å¯è§†åŒ–çš„ combined_dfï¼Œå¹¶æ‰§è¡Œåå‘æ˜ å°„ã€‚
    """
    models = {}
    scalers_X = {}
    scalers_y = {}
    metrics = {}
    
    model_dir = 'Models/Trained'

    if not os.path.exists(model_dir):
        st.error("æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„æ¨¡åž‹ã€‚è¯·å…ˆè¿è¡Œ main.pyã€‚")
        st.stop()
        
    data_dict = load_data() # data_dict ä¸­çš„ R1, R2, R3, M å·²ç»æ˜¯æ•°å€¼

    for target in get_output_variables():
        model_path = os.path.join(model_dir, f'model_{target}.joblib')
        scaler_X_path = os.path.join(model_dir, f'scaler_X_{target}.joblib')
        scaler_y_path = os.path.join(model_dir, f'scaler_y_{target}.joblib')
        
        if os.path.exists(model_path) and os.path.exists(scaler_X_path) and os.path.exists(scaler_y_path):
            models[target] = joblib.load(model_path)
            scalers_X[target] = joblib.load(scaler_X_path)
            scalers_y[target] = joblib.load(scaler_y_path)

            # ------------------------------------------------------------------
            # è®¡ç®—å¹¶å­˜å‚¨æ€§èƒ½æŒ‡æ ‡å’Œæ®‹å·®æ ‡å‡†å·® (ä½¿ç”¨æ•°å€¼åŒ–æ•°æ®)
            # ------------------------------------------------------------------
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ load_data() è¿”å›žçš„ train_x/train_yï¼Œå®ƒä»¬æ˜¯æ•°å€¼åŒ–çš„
            X_train_scaled = scalers_X[target].transform(data_dict[target]['train_x'])
            y_pred_scaled = models[target].predict(X_train_scaled)
            
            y_pred = scalers_y[target].inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            y_true = data_dict[target]['train_y'].values.flatten()
            
            residuals = y_true - y_pred
            
            metrics[target] = {
                'R2': r2_score(y_true, y_pred),
                'MAE': mean_absolute_error(y_true, y_pred),
                'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
                'Residual_Std': np.std(residuals)
            }
        else:
            st.error(f"ç¼ºå°‘ {target} çš„æ¨¡åž‹æˆ–æ ‡å‡†åŒ–å™¨æ–‡ä»¶ã€‚")
            st.stop()
            
    # ------------------------------------------------------------------
    # æž„å»ºç”¨äºŽå¯è§†åŒ–çš„ combined_df (éœ€è¦å°†åˆ†ç±»å˜é‡ä»Žæ•°å€¼åå‘æ˜ å°„å›žåç§°)
    # ------------------------------------------------------------------
    all_dfs = []
    output_vars = get_output_variables()
    
    for target in output_vars:
        # å¤åˆ¶æ•°å€¼åŒ–çš„è®­ç»ƒé›†
        train_df = data_dict[target]['train_x'].copy() 
        train_df[target] = data_dict[target]['train_y']
        
        # å…³é”®ï¼šä¸ºåˆå¹¶åšå‡†å¤‡ï¼Œç¡®ä¿å…¶ä»–ç›®æ ‡åˆ—å­˜åœ¨ä¸”ä¸º NaN
        for other_target in output_vars:
            if other_target not in train_df.columns:
                train_df[other_target] = np.nan
                
        all_dfs.append(train_df)
    
    combined_df = pd.concat(all_dfs, ignore_index=True)
    combined_df = combined_df.drop_duplicates(subset=get_input_variables()).reset_index(drop=True)
    
    # ðŸŒŸ å…³é”®ä¿®æ­£ï¼šå°† M, R1, R2, R3 çš„æ•°å€¼åå‘æ˜ å°„å›žå­—ç¬¦ä¸²
    # ... (æ­¤å¤„åº”æ˜¯ä½ ä¹‹å‰ä¿®æ­£çš„ M, R1, R2, R3 çš„åå‘æ˜ å°„ä»£ç )
    cat_mappings = get_categorical_mappings()
    categorical_vars = cat_mappings.keys()
    for var in categorical_vars:
        if var in combined_df.columns:
            inverse_map = {v: k.lower() for k, v in cat_mappings[var].items()}
            combined_df[var] = combined_df[var].map(inverse_map)
            combined_df[var] = combined_df[var].fillna('None')

    # ðŸŒŸ æ–°å¢žä¿®æ­£ï¼šå¤„ç†è¾“å‡ºå˜é‡ (CA, MW, MWD) ä¸­çš„ NaN
    # å°†è¾“å‡ºå˜é‡ä¸­çš„ NaN å¡«å……ä¸º '-' å­—ç¬¦ä¸²ï¼Œä»¥ä¾¿ Streamlit æ­£ç¡®æ˜¾ç¤º
    output_vars = get_output_variables() 
    
    for col in output_vars:
        if col in combined_df.columns:
            # 1. ç¡®ä¿å®ƒæ˜¯æ•°å€¼ (float) ç±»åž‹ï¼Œè¿™ä¼šå°†æ‰€æœ‰ç©ºå€¼ç»Ÿä¸€ä¸º np.nan
            combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce') 
            
            # 2. **å…³é”®ï¼š** ä½¿ç”¨ fillna() å°† np.nan ç»Ÿä¸€æ›¿æ¢ä¸ºå­—ç¬¦ä¸² '-'
            #    è¿™ä¸€æ­¥å°†æ‰€æœ‰æ•°å€¼ NaN è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚
            combined_df[col] = combined_df[col].fillna('-')
            
            # 3. å°†æ•´ä¸ªåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸² (å¦‚æžœè¿˜æ²¡æœ‰è½¬æ¢çš„è¯)
            combined_df[col] = combined_df[col].astype(str)
            
            # 4. **ç»ˆæžæ¸…ç†ï¼š** å°†æ‰€æœ‰ Python None å¯¹è±¡å½»åº•æ›¿æ¢ä¸ºå­—ç¬¦ä¸² '-'
            #    è¿™é’ˆå¯¹çš„æ˜¯åœ¨æ•°æ®åˆå¹¶è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½é—ç•™åœ¨ object ç±»åž‹ä¸­çš„ None å€¼ã€‚
            combined_df[col] = combined_df[col].apply(lambda x: '-' if x is None else x)

    # æ­¤æ—¶ï¼Œcombined_df çš„æ‰€æœ‰ object åˆ—éƒ½åªåŒ…å«å­—ç¬¦ä¸²ï¼Œä¸å†åŒ…å« None å¯¹è±¡ã€‚
        
    return models, scalers_X, scalers_y, combined_df, metrics

def mol_to_base64(mol):
    """å°† RDKit åˆ†å­å¯¹è±¡è½¬æ¢ä¸º Base64 å­—ç¬¦ä¸²ï¼Œä»¥ä¾¿åœ¨ Streamlit ä¸­æ˜¾ç¤ºã€‚"""
    try:
        drawer = rdMolDraw2D.MolDraw2DSVG(300, 200)
        drawer.DrawMolecule(mol)
        drawer.FinishDrawing()
        svg = drawer.GetDrawingText()
    except Exception as e:
        st.warning(f"æ— æ³•ä½¿ç”¨ GetDrawingText()ï¼Œæ­£åœ¨å°è¯• GetSVG()ã€‚é”™è¯¯: {e}")
        try:
            svg = drawer.GetSVG()
        except Exception as e2:
            st.error(f"æ— æ³•ç”ŸæˆSVGå›¾åƒã€‚é”™è¯¯: {e2}")
            return ""

    b64 = base64.b64encode(svg.encode('utf-8')).decode('utf-8')
    return f'data:image/svg+xml;base64,{b64}'

def display_ligand_images(selected_ligands):
    """å•ç‹¬çš„ç•Œé¢ç”¨äºŽæ˜¾ç¤ºæ‰€é€‰é…ä½“çš„å›¾ç‰‡"""
    st.header('å·²é€‰æ‹©é…ä½“çš„åˆ†å­ç»“æž„')
    st.write('è¯·æ£€æŸ¥ä»¥ä¸‹åˆ†å­ç»“æž„ï¼Œä»¥ç¡®ä¿å®ƒä»¬ç¬¦åˆæ‚¨çš„æœŸæœ›ã€‚')
    
    smiles_mappings = get_smiles_mappings()
    
    cols = st.columns(3)
    
    for i, var in enumerate(['R1', 'R2', 'R3']):
        with cols[i]:
            st.subheader(f'{var} é…ä½“')
            ligand_name = selected_ligands.get(var)
            
            if ligand_name and isinstance(ligand_name, str) and ligand_name in smiles_mappings.get(var, {}):
                try:
                    smiles = smiles_mappings[var][ligand_name]
                    mol = Chem.MolFromSmiles(smiles, sanitize=False)
                    if mol:
                        Chem.SanitizeMol(mol)
                        mol = Chem.RemoveHs(mol)
                        img_b64 = mol_to_base64(mol)
                        st.image(img_b64, caption=ligand_name, width='stretch')
                    else:
                        st.warning(f"æ— æ³•ä¸ºé…ä½“ '{ligand_name}' ç”Ÿæˆåˆ†å­å›¾ï¼Œå…¶ SMILES å­—ç¬¦ä¸²å¯èƒ½æ— æ•ˆã€‚")
                        st.write(f"æ— æ•ˆçš„ SMILES: `{smiles}`")
                except KeyError:
                    st.error(f"åœ¨ smiles_mappings ä¸­æ‰¾ä¸åˆ°é…ä½“ '{ligand_name}'ã€‚")
            else:
                st.info("æœªæ‰¾åˆ°åŒ¹é…çš„é…ä½“ç»“æž„ã€‚")

#----------------------------------------------------------------------------------------------------------------------------------------------------------

def get_user_input():
    st.sidebar.header('è¾“å…¥å‚æ•°')
    
    cat_mappings = get_categorical_mappings()
    
    user_inputs = {}
    selected_ligand_names = {}
    
    for var in ['R1', 'R2', 'R3']:
        st.sidebar.subheader(f'é€‰æ‹© {var} é…ä½“')
        options = list(cat_mappings[var].keys())
        selected_option_name = st.sidebar.selectbox(f"è¯·é€‰æ‹© {var} é…ä½“", options)
        selected_ligand_names[var] = selected_option_name
        user_inputs[var] = cat_mappings[var][selected_option_name]

    if 'M' in get_input_variables():
        st.sidebar.subheader('é€‰æ‹© M é‡‘å±ž')
        options = list(cat_mappings['M'].keys())
        selected_option = st.sidebar.selectbox('M', options)
        user_inputs['M'] = cat_mappings['M'][selected_option]
    
    st.sidebar.subheader('è¾“å…¥å…¶ä»–æ•°å€¼å‚æ•°')
    if 'T' in get_input_variables():
        user_inputs['T'] = st.sidebar.slider('T (Â°C)', 25.0, 250.0, 80.0)
    if 'P' in get_input_variables():
        user_inputs['P'] = st.sidebar.slider('P (MPa)', 0.1, 1.0, 0.4)
    if 'Al/M' in get_input_variables():
        user_inputs['Al/M'] = st.sidebar.slider('Al/M', 25.0, 12500.0, 2000.0)
    if 'Time' in get_input_variables():
        user_inputs['Time'] = st.sidebar.slider('Time (min)', 1.0, 60.0, 30.0)
    if 'Cat' in get_input_variables():
        user_inputs['Cat'] = st.sidebar.slider('Cat (Î¼mol)', 0.1, 5.0, 1.0)
    if 'Cocat' in get_input_variables():
        user_inputs['Cocat'] = st.sidebar.slider('Cocat (Î¼mol)', 0.1, 10.0, 2.0)
            
    ordered_inputs = {var: [user_inputs[var]] for var in get_input_variables()}
    input_df = pd.DataFrame(ordered_inputs, columns=get_input_variables())
    
    return input_df, selected_ligand_names


def main_forward_prediction(models, scalers_X, scalers_y, metrics):
    """æ­£å‘é¢„æµ‹é¡µé¢"""
    st.title('FI å‚¬åŒ–å‰‚æ€§èƒ½é¢„æµ‹åº”ç”¨')
    st.write('åœ¨å·¦ä¾§è¾“å…¥å‚æ•°ï¼Œé¢„æµ‹ä¹™çƒ¯å‡èšååº”çš„ CAã€MW å’Œ MWDã€‚')
    
    user_data, selected_ligand_names = get_user_input()
    
    st.subheader('æ‚¨çš„è¾“å…¥å‚æ•°ï¼š')
    st.write(user_data)
    
    display_ligand_images(selected_ligand_names)
    
    if st.sidebar.button('è¿›è¡Œé¢„æµ‹'):
        st.subheader('é¢„æµ‹ç»“æžœï¼š')
        results = {}
        uncertainties = {}
        for target in get_output_variables():
            scaler_X = scalers_X[target]
            scaler_y = scalers_y[target]
            
            scaled_input = scaler_X.transform(user_data)
            prediction_scaled = models[target].predict(scaled_input)
            prediction = scaler_y.inverse_transform(prediction_scaled.reshape(-1, 1))
            results[target] = prediction[0][0]
            
            # è®¡ç®—é¢„æµ‹ä¸ç¡®å®šæ€§
            uncertainty = metrics[target]['Residual_Std'] * 2  # ç®€å•ä½¿ç”¨2å€æ ‡å‡†å·®ä½œä¸º95%ç½®ä¿¡åŒºé—´
            uncertainties[target] = uncertainty
            
        unit_map = {
            'CA': 'gÂ·mmolâ»Â¹Â·hâ»Â¹',
            'MW': 'kgÂ·molâ»Â¹',
            'MWD': '(æ— å•ä½)'
        }
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(label=f"CA ({unit_map['CA']})", value=f"{results['CA']:.2f}", delta=f"Â±{uncertainties['CA']:.2f}")
        with col2:
            st.metric(label=f"MW ({unit_map['MW']})", value=f"{results['MW']:.2f}", delta=f"Â±{uncertainties['MW']:.2f}")
        with col3:
            st.metric(label=f"MWD ({unit_map['MWD']})", value=f"{results['MWD']:.2f}", delta=f"Â±{uncertainties['MWD']:.2f}")


def main_inverse_search(models, scalers_X, scalers_y):
    """
    é€†å‘å¯»æ‰¾ç›¸ä¼¼æ¡ä»¶é¡µé¢ï¼Œé€šè¿‡æ¨¡åž‹é¢„æµ‹ç”Ÿæˆè™šæ‹Ÿæ•°æ®é›†è¿›è¡Œæœç´¢ã€‚
    
    æ ¸å¿ƒé€»è¾‘ï¼šç”¨æˆ·è¾“å…¥ç›®æ ‡æ€§èƒ½ (CA, MW, MWD)ï¼Œç®—æ³•åœ¨è™šæ‹Ÿæ•°æ®é›†çš„é¢„æµ‹
    ç»“æžœä¸­ï¼Œå¯»æ‰¾æ€§èƒ½æœ€æŽ¥è¿‘ç›®æ ‡çš„ 5 ä¸ªè¾“å…¥æ¡ä»¶ã€‚
    """
    st.title('å¯»æ‰¾ç›¸ä¼¼å®žéªŒæ¡ä»¶')
    st.write('è¾“å…¥æ‚¨æœŸæœ›çš„ CAã€MW å’Œ MWDï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨å¯»æ‰¾æœ€æŽ¥è¿‘çš„æ½œåœ¨å®žéªŒæ¡ä»¶ã€‚')

    st.sidebar.header('è¾“å…¥ç›®æ ‡å€¼')

    # ç›®æ ‡æ€§èƒ½æ»‘å— (ä¿æŒä¸å˜)
    target_ca = st.sidebar.slider('ç›®æ ‡ CA', min_value=0.01, max_value=600.0, value=100.0, key='target_ca_slider')
    target_mw = st.sidebar.slider('ç›®æ ‡ MW', min_value=0.1, max_value=130.0, value=30.0, key='target_mw_slider')
    target_mwd = st.sidebar.slider('ç›®æ ‡ MWD', min_value=0.1, max_value=6.5, value=3.0, key='target_mwd_slider')

    if st.sidebar.button('å¼€å§‹å¯»æ‰¾'):
        st.subheader('æœç´¢ç»“æžœï¼š')
        
        # æ­¥éª¤ 1: ç”Ÿæˆè™šæ‹Ÿæ•°æ®é›†
        st.info("æ­£åœ¨åˆ©ç”¨æ¨¡åž‹ç”Ÿæˆè™šæ‹Ÿå®žéªŒæ¡ä»¶æ•°æ®é›†ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
        
        cat_mappings = get_categorical_mappings()
        # smiles_mappings = get_smiles_mappings() # æœªä½¿ç”¨ï¼Œå¯åˆ é™¤

        # ç»„åˆæ‰€æœ‰å¯èƒ½çš„ç¦»æ•£å˜é‡
        discrete_vars_combos = list(itertools.product(
            list(cat_mappings['M'].keys()),
            list(cat_mappings['R1'].keys()),
            list(cat_mappings['R2'].keys()),
            list(cat_mappings['R3'].keys())
        ))

        # ðŸŒŸ å…³é”®ä¿®æ­£ï¼šåˆ›å»ºæ›´ç²¾ç»†çš„æ•°å€¼å˜é‡ç½‘æ ¼ï¼Œå¢žåŠ ç»“æžœå¤šæ ·æ€§
        temp_grid = np.linspace(25, 250, 10)  # ä»Ž 10 å¢žåŠ åˆ° 20
        press_grid = np.linspace(0.1, 1.0, 5) # ä»Ž 5 å¢žåŠ åˆ° 10
        al_m_grid = np.linspace(25, 12500, 10) # ä»Ž 10 å¢žåŠ åˆ° 20
        time_grid = np.linspace(1, 60, 5)    # ä»Ž 5 å¢žåŠ åˆ° 10
        
        numerical_vars_combos = list(itertools.product(temp_grid, press_grid, al_m_grid, time_grid))

        # æž„å»ºè™šæ‹Ÿè¾“å…¥ DataFrame
        virtual_inputs = []
        for discrete_combo in discrete_vars_combos:
            for numerical_combo in numerical_vars_combos:
                row = {
                    'M': cat_mappings['M'][discrete_combo[0]],
                    'R1': cat_mappings['R1'][discrete_combo[1]],
                    'R2': cat_mappings['R2'][discrete_combo[2]],
                    'R3': cat_mappings['R3'][discrete_combo[3]],
                    'T': numerical_combo[0],
                    'P': numerical_combo[1],
                    'Al/M': numerical_combo[2],
                    'Time': numerical_combo[3],
                    'Cat': 1.0, # å‡è®¾ Cat å’Œ Cocat ä¸ºå¸¸æ•°
                    'Cocat': 2.0
                }
                virtual_inputs.append(row)

        virtual_df = pd.DataFrame(virtual_inputs, columns=get_input_variables())
        
        # æ­¥éª¤ 2: é¢„æµ‹è™šæ‹Ÿæ•°æ®é›†çš„æ€§èƒ½
        virtual_df_with_preds = virtual_df.copy()
        for target in get_output_variables():
            # è¿™é‡Œçš„æ¨¡åž‹å’Œæ ‡é‡éœ€è¦ä»Ž load_models_and_data å‡½æ•°ä¼ å…¥
            scaler_X = scalers_X[target]
            model = models[target]
            
            scaled_input = scaler_X.transform(virtual_df)
            y_pred_scaled = model.predict(scaled_input)
            y_pred = scalers_y[target].inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            virtual_df_with_preds[target] = y_pred

        st.success(f"å·²ç”Ÿæˆ {len(virtual_df_with_preds)} ä¸ªè™šæ‹Ÿå®žéªŒæ¡ä»¶ã€‚")
        
        # æ­¥éª¤ 3: å¯»æ‰¾æœ€æŽ¥è¿‘ç›®æ ‡å€¼çš„æ¡ä»¶ï¼ˆçº¯è¾“å‡ºç©ºé—´ç›¸ä¼¼æ€§æœç´¢ï¼‰
        
        combined_df_normalized = virtual_df_with_preds.copy()
        target_values = {}
        output_vars = get_output_variables()
        
        # å½’ä¸€åŒ–è¾“å‡ºå˜é‡ (CA, MW, MWD)
        for var in output_vars:
            mean = virtual_df_with_preds[var].mean()
            std = virtual_df_with_preds[var].std()
            
            # ðŸŒŸ å…³é”®ï¼šå¯¹æ ‡å‡†å·®ä¸º0çš„æƒ…å†µè¿›è¡Œå¤„ç†ï¼Œé¿å…é™¤ä»¥0
            if std == 0 or np.isnan(std):
                combined_df_normalized[var] = 0
                target_values[var] = 0
            else:
                combined_df_normalized[var] = (virtual_df_with_preds[var] - mean) / std

        # å½’ä¸€åŒ–ç›®æ ‡å€¼
        target_values['CA'] = (target_ca - virtual_df_with_preds['CA'].mean()) / virtual_df_with_preds['CA'].std()
        target_values['MW'] = (target_mw - virtual_df_with_preds['MW'].mean()) / virtual_df_with_preds['MW'].std()
        target_values['MWD'] = (target_mwd - virtual_df_with_preds['MWD'].mean()) / virtual_df_with_preds['MWD'].std()
        
        # 4. è®¡ç®—è·ç¦» (åªé’ˆå¯¹å½’ä¸€åŒ–åŽçš„ CA, MW, MWD)
        normalized_data_matrix = combined_df_normalized[['CA', 'MW', 'MWD']].values
        target_vector = np.array([target_values['CA'], target_values['MW'], target_values['MWD']])
        
        distances = np.linalg.norm(normalized_data_matrix - target_vector, axis=1)
        virtual_df_with_preds['distance'] = distances
        
        # 5. æŽ’åºå’Œæå–ç»“æžœ
        closest_matches = virtual_df_with_preds.sort_values(by='distance').head(5)
        
        inverse_map = {col: {v: k for k, v in cat_mappings[col].items()} for col in ['M', 'R1', 'R2', 'R3']}

        st.success('æ‰¾åˆ°ä»¥ä¸‹5ä¸ªæœ€æŽ¥è¿‘çš„æ½œåœ¨å®žéªŒæ¡ä»¶ï¼š')
        
        # 6. å±•ç¤ºç»“æžœ
        for i, (index, row) in enumerate(closest_matches.iterrows()):
            st.markdown(f"**--- ç¬¬ {i+1} ä¸ªæœ€æŽ¥è¿‘çš„æ¡ä»¶ ---**")
            
            # å‡†å¤‡è¦å±•ç¤ºçš„ DataFrame
            result_df = row.drop(['CA', 'MW', 'MWD', 'distance']).to_frame().T
            
            # é€†å‘æ˜ å°„ç¦»æ•£å˜é‡
            for col in ['R1', 'R2', 'R3', 'M']:
                if col in result_df.columns:
                    value = float(row[col])
                    # ä½¿ç”¨ .get æ–¹æ³•é¿å… KeyError
                    ligand_name = inverse_map[col].get(value, f"æœªçŸ¥é…ä½“({value})") 
                    result_df[col] = ligand_name
            
            # æ ¼å¼åŒ– T, P, Al/M, Time ç­‰æ•°å€¼
            result_df['T'] = result_df['T'].round(1)
            result_df['P'] = result_df['P'].round(2)
            result_df['Al/M'] = result_df['Al/M'].round(0).astype(int)
            result_df['Time'] = result_df['Time'].round(1)
            
            st.dataframe(result_df)
            
            unit_map = {
                'CA': 'gÂ·mmolâ»Â¹Â·hâ»Â¹',
                'MW': 'kgÂ·molâ»Â¹',
                'MWD': '(æ— å•ä½)'
            }
            st.markdown(f"**è¯¥æ¡ä»¶çš„é¢„æµ‹æ€§èƒ½ï¼š**")
            st.info(f"CA: {row['CA']:.2f} {unit_map['CA']} | MW: {row['MW']:.2f} {unit_map['MW']} | MWD: {row['MWD']:.2f} {unit_map['MWD']}")
            
            with st.expander(f"æŸ¥çœ‹è¯¥æ¡ä»¶çš„é…ä½“åˆ†å­ç»“æž„"):
                # é€†å‘æ˜ å°„ç¦»æ•£å˜é‡çš„åç§°ç”¨äºŽç»“æž„å±•ç¤º
                selected_ligand_names = {
                    'R1': inverse_map['R1'].get(float(row['R1']), "æœªçŸ¥"),
                    'R2': inverse_map['R2'].get(float(row['R2']), "æœªçŸ¥"),
                    'R3': inverse_map['R3'].get(float(row['R3']), "æœªçŸ¥")
                }
                display_ligand_images(selected_ligand_names)
                
def main_model_performance(metrics):
    """æ¨¡åž‹æ€§èƒ½å±•ç¤ºé¡µé¢"""
    st.title('æ¨¡åž‹æ€§èƒ½è¯„ä¼°')
    st.write('ä»¥ä¸‹æ˜¯æ¯ä¸ªé¢„æµ‹æ¨¡åž‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¡¨çŽ°ã€‚')
    
    unit_map = {
        'CA': 'gÂ·mmolâ»Â¹Â·hâ»Â¹',
        'MW': 'kgÂ·molâ»Â¹',
        'MWD': '(æ— å•ä½)'
    }
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.header('CA')
        st.metric(label="RÂ² Score", value=f"{metrics['CA']['R2']:.4f}")
        st.metric(label=f"RMSE ({unit_map['CA']})", value=f"{metrics['CA']['RMSE']:.2f}")
        st.metric(label=f"MAE ({unit_map['CA']})", value=f"{metrics['CA']['MAE']:.2f}")
    with col2:
        st.header('MW')
        st.metric(label="RÂ² Score", value=f"{metrics['MW']['R2']:.4f}")
        st.metric(label=f"RMSE ({unit_map['MW']})", value=f"{metrics['MW']['RMSE']:.2f}")
        st.metric(label=f"MAE ({unit_map['MW']})", value=f"{metrics['MW']['MAE']:.2f}")
    with col3:
        st.header('MWD')
        st.metric(label="RÂ² Score", value=f"{metrics['MWD']['R2']:.4f}")
        st.metric(label=f"RMSE ({unit_map['MWD']})", value=f"{metrics['MWD']['RMSE']:.2f}")
        st.metric(label=f"MAE ({unit_map['MWD']})", value=f"{metrics['MWD']['MAE']:.2f}")

def main_data_visualization(combined_df):
    """æ•°æ®æŽ¢ç´¢ä¸Žå¯è§†åŒ–é¡µé¢"""
    st.title('æ•°æ®æŽ¢ç´¢ä¸Žå¯è§†åŒ–')
    st.write('é€šè¿‡é€‰æ‹©å˜é‡ï¼ŒæŽ¢ç´¢ä¸åŒå‚æ•°å¯¹å‚¬åŒ–å‰‚æ€§èƒ½çš„å½±å“ã€‚')
    
        # ä¸´æ—¶è¯Šæ–­ï¼šæ˜¾ç¤ºæ•°æ®æ¡†ï¼Œç¡®è®¤æ•°æ®å·²åŠ è½½
    st.write("è¯Šæ–­: combined_df çš„æ•°æ®ç±»åž‹")
    st.write(combined_df.dtypes)

    # è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
    plt.rcParams['font.sans-serif'] = ['SimHei']  # æˆ–è€…ä½¿ç”¨ 'WenQuanYi Micro Hei'
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

    categorical_vars = list(get_categorical_mappings().keys())

    all_vars = get_input_variables() + get_output_variables()
    
    st.sidebar.subheader('ç»˜å›¾è®¾ç½®')
    x_axis = st.sidebar.selectbox('é€‰æ‹© X è½´å˜é‡', all_vars, index=0)
    y_axis = st.sidebar.selectbox('é€‰æ‹© Y è½´å˜é‡', get_output_variables(), index=0)
    
    # âš ï¸ ä¿®å¤ï¼šä¿®æ­£å˜é‡ç±»åž‹åˆ¤æ–­é€»è¾‘
    is_x_categorical = x_axis in categorical_vars
    is_y_categorical = y_axis in categorical_vars # å¯¹äºŽè¾“å‡ºå˜é‡ï¼Œè¿™åº”è¯¥å§‹ç»ˆä¸º Falseï¼Œå› ä¸ºå®ƒä¸æ˜¯åˆ†ç±»å˜é‡
    
    # ä¸´æ—¶è¯Šæ–­ï¼šæ˜¾ç¤ºæ•°æ®æ¡†ï¼Œç¡®è®¤æ•°æ®å·²åŠ è½½
    st.dataframe(combined_df.head())

    fig, ax = plt.subplots(figsize=(10, 6))

    if not is_x_categorical and not is_y_categorical:
        # æ•£ç‚¹å›¾
        sns.scatterplot(data=combined_df, x=x_axis, y=y_axis, ax=ax)
        ax.set_title(f'{y_axis} vs. {x_axis} æ•£ç‚¹å›¾')
        st.pyplot(fig)
    elif is_x_categorical and not is_y_categorical:
        # å°æç´å›¾/ç®±çº¿å›¾
        sns.violinplot(data=combined_df, x=x_axis, y=y_axis, ax=ax, inner="quartile")
        sns.stripplot(data=combined_df, x=x_axis, y=y_axis, ax=ax, color='black', alpha=0.5, jitter=True)
        ax.set_title(f'{y_axis} by {x_axis} å°æç´å›¾')
        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
        plt.tight_layout()
        st.pyplot(fig)
    elif not is_x_categorical and is_y_categorical:
        st.warning("è¯·é€‰æ‹©ä¸€ä¸ªæ•°å€¼å˜é‡ä½œä¸º Y è½´ã€‚")
    else:
        st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ•°å€¼å˜é‡è¿›è¡Œç»˜å›¾ã€‚")

def main():
    st.sidebar.title("åŠŸèƒ½é€‰æ‹©")
    app_mode = st.sidebar.radio("è¯·é€‰æ‹©ä¸€ä¸ªåŠŸèƒ½", ["æ­£å‘é¢„æµ‹", "å¯»æ‰¾ç›¸ä¼¼æ¡ä»¶", "æ¨¡åž‹æ€§èƒ½", "æ•°æ®å¯è§†åŒ–"])

    models, scalers_X, scalers_y, combined_df, metrics = load_models_and_data()

    if app_mode == "æ­£å‘é¢„æµ‹":
        main_forward_prediction(models, scalers_X, scalers_y, metrics)
    elif app_mode == "å¯»æ‰¾ç›¸ä¼¼æ¡ä»¶":
        # ä¼ é€’å¿…è¦çš„å‚æ•°ç»™æ–°çš„ inverse_search å‡½æ•°
        main_inverse_search(models, scalers_X, scalers_y)
    elif app_mode == "æ¨¡åž‹æ€§èƒ½":
        main_model_performance(metrics)
    elif app_mode == "æ•°æ®å¯è§†åŒ–":
        main_data_visualization(combined_df)


# æœ€ç»ˆä¿®å¤åŽçš„ä¸»å…¥å£ç‚¹
if __name__ == '__main__':

    main() 







